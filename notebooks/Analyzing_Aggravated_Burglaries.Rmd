---
title: "Analyzing Aggravated Burglaries in Davidson County"
output: html_notebook
---

# Data Preparation-------------------------------------------------------------------------------------------
```{r}
library(httr)
library(jsonlite)
library(sf)
library(tidyverse)
library(leaflet)
library(usethis)
library(janitor)
```


```{r}
base_url <- "https://services2.arcgis.com/HdTo6HJqh92wn4D8/arcgis/rest/services/Metro_Nashville_Police_Department_Incidents_view/FeatureServer/0/query?where=1=1&outFields=*&f=json"

params <- list(
  where = "offense_description LIKE '%BURGLARY%AGGRAVATED' AND Incident_Occurred >= '2023-01-01 00:00:00' AND Incident_Occurred <= '2023-12-31 23:59:59'",
  outFields = 'primary_key,incident_number,report_type,report_type_description,incident_status_code,incident_status_description,investigation_status,incident_occurred,incident_reported,incident_location,latitude,longitude,zip_code,location_code,location_description,offense_number,offense_nibrs,offense_description,weapon_primary,weapon_description,victim_number,domestic_related,victim_type,victim_description,mapped_location,victim_gender,victim_race,victim_ethnicity,victim_county_resident',
  f = 'json'
)

url_obj <- parse_url(base_url)
url_obj$query <- params
full_url <- build_url(url_obj)

response <- GET(full_url)
data_parsed <- fromJSON(content(response, 'text', encoding = 'UTF-8'))

burglaries <- data_parsed$features$attributes |>
  mutate(Incident_Occurred = as.POSIXct(Incident_Occurred/1000, tz='America/Chicago'),
         Incident_Reported = as.POSIXct(Incident_Reported/1000, tz='America/Chicago'))

head(burglaries)
```
Getting Davidson county census tract populations:
```{r}
api_key <- Sys.getenv("MY_API_KEY")
base_url <- sprintf("https://api.census.gov/data/2023/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:47&in=county:037&key=%s", api_key)

response <- GET(base_url)
data_parsed <- fromJSON(content(response, 'text', encoding = 'UTF-8'))

census_pop <- as_tibble(data_parsed) |> row_to_names(row_number = 1) |> rename(population=B01001_001E) |> mutate(tract = as.numeric(tract), county = as.numeric(county), state = as.numeric(state), population = as.numeric(population))

head(census_pop)
```

```{r}
api_key <- Sys.getenv("MY_API_KEY")
base_url <- sprintf("https://api.census.gov/data/2023/acs/acs5/subject?get=NAME,S1901_C01_012E&for=tract:*&in=state:47&in=county:037&key=%s", api_key)

response <- GET(base_url)
data_parsed <- fromJSON(content(response, 'text', encoding = 'UTF-8'))

census_income <- as_tibble(data_parsed) |> row_to_names(row_number = 1) |> rename(median_income=S1901_C01_012E) |> mutate(tract = as.numeric(tract), county = as.numeric(county), state = as.numeric(state), median_income = as.numeric(median_income))

head(census_income)
```



```{r}
census <- full_join(census_pop, census_income, by=c('NAME', 'tract', 'state', 'county'))

census
```
Note that census tracts 980100 and 980200 seem to have bad data, with recorded populations of 0 and negative median incomes. 
```{r}
census |>
  filter(population==0)
```
We'll replace those values with NA.
```{r}
census$population <- na_if(census$population, 0)
census$median_income <- na_if(census$median_income, -666666666)
```


Reading in Davidson County census tract shapefile:
```{r}
DC_tracts <- st_read('../data/DC/DC.shp') |>
  # Renaming tracts column and converting to numeric datatype to simplify future merging and plots
  rename(tract=TRACTCE) |>
  mutate(tract=as.numeric(tract))
DC_tracts
```

Adding land area to the census dataframe and calculating population density for future use:
```{r}
census <- DC_tracts |>
  st_drop_geometry() |>
  rename(land_area = ALAND) |>
  select(land_area, tract) |>
  full_join(census, by='tract') |>
  # Multiplication factor is 1,000,000 since there are 1 mil square meters in one square kilometer
  mutate(pop_density = population/land_area*1000000)

head(census)
```


We can create a plot of our geospatial census tract data using ggplot. 
```{r}
DC_tracts |> 
  ggplot() +
  geom_sf()
```

Now we will perform a spatial join to determine the census tract in which each burglary occurred. 
```{r}
# Adding spatial geometry to burglaries data
burglaries_geo <- st_as_sf(
  burglaries |> drop_na(Latitude),
  coords = c('Longitude', 'Latitude'),
  crs = st_crs(DC_tracts),
  remove = FALSE
)

# Spatial join to determine the census tract associated with each burglary
# Putting the tracts on the left makes sure that the final dataset contains all census tracts, even those with zero burglaries
# It also removes any burglaries that our outside of Davidson County
burglaries_by_tract <- 
  st_join(DC_tracts, burglaries_geo, join = st_contains)

burglaries_by_tract |>
  st_drop_geometry()
```

# Exploratory analysis-------------------------------------------------------------------------------------------

Leaflet cluster map of burglaries in Nashville:
```{r}
leaflet(burglaries_by_tract |> drop_na(Longitude)) |> 
  addTiles() |> 
  addMarkers(
    ~Longitude,
    ~Latitude,
    clusterOptions = markerClusterOptions()
  )
```

 
Which census tract had the highest number of burglaries? Which census tract had the highest number of burglaries per 1000 residents?
```{r}
burglary_totals <- burglaries_by_tract |> 
  st_drop_geometry() |> 
  group_by(tract) |> 
  summarize(Num_Unique_Burglaries = n_distinct(Incident_Number, na.rm=TRUE)) |>
  ungroup() |>
  inner_join(census, by='tract') |>
  mutate(Burglaries_Per_Pop = Num_Unique_Burglaries/population*1000) |>
  select(tract, Num_Unique_Burglaries, Burglaries_Per_Pop, population, median_income, pop_density) |>
  arrange(desc(Num_Unique_Burglaries))

burglary_totals
```
```{r}
burglary_totals |>
  slice_max(Num_Unique_Burglaries)
```
```{r}
burglary_totals |>
  slice_max(Burglaries_Per_Pop)
```
We see that tract 16000 has the largest number of burglaries and also the largest rate of burglaries per 1000 residents.

Plotting burglaries for tract 16000:

```{r}
tract_code = 16000

DC_tracts |> 
  filter(tract == tract_code) |>
  ggplot() +
  geom_sf() +
  geom_point(data = burglaries_by_tract |> filter(tract == tract_code), aes(x = Longitude, y = Latitude)) +
  labs(title=paste('Map of Burglaries in Census Tract',tract_code), subtitle='Davidson County, 2023')
```
Plot of Nashville census tracts shaded by burglary rate:
```{r}
# Merge burglary rates into DC_tracts so that we can incorporate things like burglary rates and median income into our plots
DC_tracts_info <- full_join(DC_tracts, burglary_totals, by='tract')
DC_tracts_info
```


```{r, fig.height=9, fig.width=9}
DC_tracts_info |> 
  ggplot() +
  geom_sf(aes(fill = Burglaries_Per_Pop)) +
  scale_fill_viridis_c(option = "plasma", direction = -1, alpha=0.8) + 
  labs(title= 'Burglary Rates in Davidson County', subtitle = 'Tracts Shaded by Burglaries per Thousand Residents, 2023') +
  theme_minimal()
```

```{r, fig.height=9, fig.width=9}
DC_tracts_info |> 
  ggplot() +
  geom_sf(aes(fill = Num_Unique_Burglaries)) +
  scale_fill_viridis_c(option = "plasma", direction = -1, alpha=0.8) + 
  labs(title= 'Burglary Totals in Davidson County', subtitle = 'Tracts Shaded by Burglaries Total, 2023') +
  theme_minimal()
```

Plot of burglaries in Nashville with tracts shaded by median income:

```{r, fig.height=9, fig.width=9}
DC_tracts_info |> 
  ggplot() +
  geom_sf(aes(fill = median_income)) +
  scale_fill_viridis_c(option = "plasma", direction = -1, alpha=0.8) + 
  geom_point(data = burglaries_by_tract |> drop_na(Incident_Number), aes(x = Longitude, y = Latitude), size = 0.5) +
  labs(title= 'Burglaries in Davidson County', subtitle = 'Tracts Shaded by Median Income') +
  theme_minimal()
```

There do seem to be a higher concentration of burglaries in lower-income tracts. We can investigate the strength of this correlation with a Poisson regression in the statistical modeling section below. 

Plot of burglaries in Nashville with tracts shaded by population density:

```{r, fig.height=9, fig.width=9}
DC_tracts_info |> 
  ggplot() +
  geom_sf(aes(fill = pop_density)) +
  scale_fill_viridis_c(option = "plasma", direction = -1, alpha=0.8) + 
  geom_point(data = burglaries_by_tract |> drop_na(Incident_Number), aes(x = Longitude, y = Latitude), size = 0.5) +
  labs(title= 'Burglaries in Davidson County', subtitle = 'Tracts Shaded by Population Density') +
  theme_minimal()
```
There does seem to be a relationship between population density and number of burglaries. We can incorporate that into our Poisson regression model as well. 

```{r}
# Scatterplot showing burglary rate vs median income
burglary_totals |>
  ggplot(aes(x=median_income, y=Burglaries_Per_Pop)) +
  geom_point() + 
  labs(x='Median Income', y='Burglaries per Thousand Residents', title='Burglary Rate versus Median Income', subtitle='For Davidson County Census Tracts, 2023')
```
The relationship between median income and burglary rate appears to be a curve, indicating that a Poisson model might be appropriate, since it is a model of log(burglary rate).

```{r}
# Scatterplot showing burglary rate vs population density
burglary_totals |>
  ggplot(aes(x=pop_density, y=Burglaries_Per_Pop)) +
  geom_point() + 
  labs(x='Population Density', y='Burglaries per Thousand Residents', title='Burglary Rate versus Population Density', subtitle='For Davidson County Census Tracts, 2023')
```
There appears to be very little correlation between population density and burglary rate when we look at the scatterplot. 

To further investigate the assumptions of a Poisson model, we can look at the distribution of burglary rate in more detail. 

```{r}
burglary_totals |>
  ggplot(aes(x=Burglaries_Per_Pop)) +
  geom_histogram(fill='dodgerblue',color='black') + 
  labs(x='Burglaries per Thousand Residents', y='Count', title='Distribution of Burglary Rates', subtitle='For Davidson County Census Tracts, 2023')
```
The above histogram shows a fair amount of variability in burglary rate; rates range from roughly 0 to 16 with most of the rates clustered between 0 and 5. The distribution is right skewed, like many Poisson distributions, rather than being normally distributed (as we would expect with a linear regression model). 

We can also look at the distribution of burglary rates faceted by different ranges of median income. 

```{r}
# Convert from scientific notation to decimal notation
options(scipen = 999)

# Breaking the median income observations into categories
burglary_totals$median_income_bins <- cut(burglary_totals$median_income, breaks = 12, dig.lab = 6)

# Plotting the distribution of burglary rates by each bin of median income
burglary_totals |>
  ggplot(aes(x = Burglaries_Per_Pop)) +
  geom_histogram(binwidth = 1, fill = "dodgerblue", color = "black") +
  facet_wrap(~ median_income_bins, ncol = 2) +
  labs(
    title = "Burglary Rate Distribution Faceted by Median Income",
    x = "Burglary Rate",
    y = "Frequency"
  ) +
  theme_minimal()
```
If a Poisson model were appropriate, we would expect the distribution of burglary rate to be right-skewed for lower median incomes, and transition to more of a normal distribution as median income increases. For this dataset, we have a roughly uniform distribution for the lowest median income bucket. For the next highest median income bucket, we see a definite right skew in the burglary distribution. We have too few observations for the higher income buckets to discern a shape in the distribution. This requirement of the Poisson model does not seem to be met by our data. 

```{r}
# Comparing mean and variance of burglary rate within each median income bucket
burglary_stats_by_income <- burglary_totals |> 
  group_by(median_income_bins) |>
  summarize(
    Mean_Burglaries = mean(Burglaries_Per_Pop, na.rm = TRUE),
    Variance_Burglaries = var(Burglaries_Per_Pop, na.rm = TRUE),
    Num_Obs = n()
  ) |>
  mutate(
  left_endpoint = as.numeric(str_extract(median_income_bins, "\\((.+),", group = 1)),
  right_endpoint = as.numeric(str_extract(median_income_bins, ",(.+)]", group = 1)),
  income_midpoint = right_endpoint-left_endpoint/2
  )

burglary_stats_by_income
```
We see that the for the lowest median income bucket, the variance (~17) is very different than the mean (~4.5), which violates one of the assumptions of a Poisson model (mean=variance for the target variable), possibly making it unfit for a Poisson regression model.

Finally, another assumption of a Poisson model is that the log of the mean of the target variable is a linear function of the explanatory variable. We can assess this linearity assumption by looking at a plot of the logs of the empirical average burglary rates and plotting by median income.

```{r}
burglary_stats_by_income |>
  ggplot(aes(x=income_midpoint, y=log(Mean_Burglaries))) +
  geom_point() + 
  geom_smooth(method='glm') +
  labs(title='Log of Burglary Rate vs Median Income', subtitle='For Davidson County Census Tracts, 2023', y='Log of the Empirical Mean Burglary Rate', x='Median Income')
```
Median income and the log of burglary rate appears to have a weak linear association. 

# Statistical Modeling-------------------------------------------------------------------------------------------
```{r}
burglaries_income_pop <- glm("Num_Unique_Burglaries ~ median_income",
                    data = burglary_totals,
                    family = "poisson",
                    offset = log(population))

summary(burglaries_income_pop)
```


```{r}
# Adding model predictions to our scatterplot showing burglary rate vs median income
pred_df <- tibble(median_income = seq(from = 0, to = max(burglary_totals$median_income, na.rm = TRUE), length.out = 200),
population = 1000)

pred_df$estimate <- predict(burglaries_income_pop, newdata = pred_df, type = 'response')

burglary_totals |>
  ggplot(aes(x=median_income, y=Burglaries_Per_Pop)) +
  geom_point() + 
  geom_line(data=pred_df, aes(x=median_income, y=estimate)) +
  labs(x='Median Income', y='Burglaries per Thousand Residents', title='Poisson Regression for Burglary Rate versus Median Income', subtitle='For Davidson County Census Tracts, 2023')
```
We can also look at the distribution of predicted probabilities for various values of median income, to see if it resembles a Poisson distribution.

```{r}
median_income <- seq(from = 20000, to = 180000, length.out = 5)
population = 4000

map(median_income, 
    \(x) tibble(median_income = x, 
                num_burglaries = 0:20, 
                probability = dpois(0:20, 
                                      lambda = predict(burglaries_income_pop,
                                                   newdata = tibble(median_income = x, population = population), type = "response")
                                    )
                )
    ) |> 
  bind_rows() |> 
  ggplot(aes(x = num_burglaries, y = probability)) +
  geom_col() +
  facet_wrap(~median_income)
```
As the average number of burglaries increases, we see the distribution move from a right skewed distribution to more of a normal distribution. 

```{r}
burglaries_income_pop_coefs <- burglaries_income_pop |> coef() |> exp() 
burglaries_income_pop_coefs
```

```{r}
# Multiplying coefs by 10,000 before exponentiating to get a better real-world idea of the influence
adjusted_coefs <- burglaries_income_pop |> 
                      coef() |>
                      (\(b) exp(b * 10000))()
adjusted_coefs
```


```{r}
pct_change <- round((1-adjusted_coefs['median_income'])*100, 4)

paste0('For 2 Davidson county census tracts with the same population, an increase in median income of $10,000 is associated with a ',pct_change,'% lower estimated average number of burglaries.')
```

```{r}
burglaries_income_pop_density <- glm("Num_Unique_Burglaries ~ median_income + pop_density",
                    data = burglary_totals,
                    family = "poisson",
                    offset = log(population))

summary(burglaries_income_pop_density)
```
Adding population density to the model lowers the AIC slightly, from 1304.4 to 1303.3, which indicates it improves the model's ability to explain the variation in burglary rates. 

       
```{r}
burglaries_income_pop_density_coefs <- burglaries_income_pop_density |> coef() |> exp() 
burglaries_income_pop_density_coefs
```

```{r}
# Multiplying coefs by a scaling factor before exponentiating to get a better real-world idea of the influence
adjusted_med_income_coef <- burglaries_income_pop_density |> 
                      coef() |> (\(x) x['median_income'])() |> (\(b) exp(b * 10000))()

adjusted_pop_dens_coef <- burglaries_income_pop_density |> 
                      coef() |> (\(x) x['pop_density'])() |> (\(b) exp(b * 100))()

adjusted_med_income_coef
adjusted_pop_dens_coef
```
```{r}
pct_change_income <- round((1-adjusted_med_income_coef)*100, 4)
pct_change_density <- round((adjusted_pop_dens_coef-1)*100, 4)

paste0('For 2 Davidson county census tracts with the same population and the same population density, an increase in median income of $10,000 is associated with a ',pct_change_income,'% lower estimated average number of burglaries.')

paste0('For 2 Davidson county census tracts with the same population and the same median income, an increase in population density of 100 people per square kilometer is associated with a ',pct_change_density,'% higher estimated average number of burglaries.')
```

Evaluating how well the model fits the data:

```{r}
exp(confint(burglaries_income_pop_density))
```
When we create 95% confidence intervals for the exponentiated coefficients, we see the interval for median income does not include 1, suggesting it is significantly associated with burglary rate. The confident interval for population density does include 1, indicating population density is not significantly associated with burglary rate.


```{r}
# Comparing model predictions to observed rates

# Get the burglary rate values that the model predicts
fitted_values <- predict(burglaries_income_pop_density, type = "response")
residuals <- residuals(burglaries_income_pop_density, type = "response")
observed_values <- burglary_totals |>
  filter(!is.na(median_income)) |>
  select(Num_Unique_Burglaries) |>
  pull()

predicted_vs_observed <- tibble(predicted_burglaries=fitted_values, observed_burglaries=observed_values, residual=residuals)
```

```{r}
# Plotting observed vs. predicted burglary numbers
predicted_vs_observed |>
  ggplot(aes(x=predicted_burglaries, y=observed_burglaries)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed", size = 1) + # Add a y=x line, which would indicate observed=predicted 
  labs(x = "Predicted Burglary Rates",
     y = "Observed Burglary Rates",
     title = "Observed vs. Predicted Burglary Rates")
```

```{r}
# Plotting residuals vs. predicted burglary numbers
predicted_vs_observed |>
  ggplot(aes(x=predicted_burglaries, y=residual)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, color = "red", linetype = "dashed", size = 1) + # Add a y=0 line, which would indicate observed=predicted 
  labs(x = "Predicted Burglary Rates",
     y = "Residual",
     title = "Residuals vs. Predicted Burglary Rates")
```
We can see that the model does a poor job of predicting burglary rates, especially for high observed burglary rates. When observed burglary rates were greater than 20, the model tended to greatly underestimate the actual burglary rate. 

```{r}
# Extract the residual deviance and degrees of freedom
deviance <- summary(burglaries_income_pop_density)$deviance
degrees_freedom <- summary(burglaries_income_pop_density)$df.residual

# Calculate the p-value for the deviance test
p_value_deviance <- 1 - pchisq(deviance, degrees_freedom)

paste("Residual Deviance:", deviance)
paste("Degrees of Freedom:", degrees_freedom)
paste("P-value:", p_value_deviance)
```

The small p-value (0 < 0.05) returned by the deviance test suggests that our Poisson regression model is a poor fit. This could be due to overdispersion, where the variance of our observed values is greater than the mean at each level of the explanatory variable.

To test for overdispersion, we can run a dispersion test.

```{r}
library(AER)
library(MASS)
```


```{r}
dispersiontest(burglaries_income_pop_density)
```
Our p-value is less than alpha=0.05, so we reject the null hypothesis and conclude that the true dispersion of burglary counts is greater than 1. 


Since we have good evidence our data is overdispersed, we can run a negative binomial regression model to account for this.
```{r}
burg_income_pop_dens_nb <- glm.nb(Num_Unique_Burglaries ~ median_income + pop_density + offset(log(population)),
                    data = burglary_totals)

summary(burg_income_pop_dens_nb)
```
The p-value for median income is statistically significant (using alpha=0.05); the p-value for population density is not statistically significant (using alpha=0.05). 

```{r}
burg_income_pop_dens_nb$theta
```
The theta parameter in our negative binomial model is used to correct for the overdispersion in our response variable.

We can look at the distribution of predicted probabilities for various values of median incomes, to see if it resembles a Poisson distribution.
```{r}
median_income <- seq(from = 20000, to = 180000, length.out = 5)
pop_density = 1200
population = 4000

map(median_income, 
    \(x) tibble(median_income = x, 
                num_burglaries = 0:20, 
                probability = dnbinom(0:20, 
                                      mu = predict(burg_income_pop_dens_nb,
                                      newdata = tibble(median_income = x, population = population, pop_density = pop_density), type = "response"),
                                      size = burg_income_pop_dens_nb$theta
                                    )
                )
    ) |> 
  bind_rows() |> 
  ggplot(aes(x = num_burglaries, y = probability)) +
  geom_col() +
  facet_wrap(~median_income)
```
